{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the terminal, paste the following: \n",
    "# python -m pip install -U pydantic spacy==3.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy is more useful in the development and production environment because \n",
    "# it provides a very fast and accurate semantic analysis compared to NLTK.\n",
    "\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"en_core_web_sm\" is spaCy's small English pipeline trained on written web text \n",
    "# (blogs, news, comments), that includes vocabulary, syntax and entities.\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "\n",
    "########## Process Text ##########\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'when': 1, 'sebastian': 1, 'thrun': 2, 'started': 1, 'working': 1, 'on': 1, 'self': 1, '-': 1, 'driving': 1, 'cars': 1, 'at': 1, 'google': 1, 'in': 2, '2007': 1, ',': 3, 'few': 1, 'people': 1, 'outside': 1, 'of': 2, 'the': 1, 'company': 1, 'took': 1, 'him': 1, 'seriously': 1, '.': 2, '“': 1, 'i': 2, 'can': 1, 'tell': 1, 'you': 1, 'very': 1, 'senior': 1, 'ceos': 1, 'major': 1, 'american': 1, 'car': 1, 'companies': 1, 'would': 1, 'shake': 1, 'my': 1, 'hand': 1, 'and': 1, 'turn': 1, 'away': 1, 'because': 1, 'was': 1, 'n’t': 1, 'worth': 1, 'talking': 1, 'to': 1, '”': 1, 'said': 1, 'an': 1, 'interview': 1, 'with': 1, 'recode': 1, 'earlier': 1, 'this': 1, 'week': 1}\n"
     ]
    }
   ],
   "source": [
    "########## Counting Number of Words ##########\n",
    "word_dict = {}\n",
    "\n",
    "for word in doc:\n",
    "    word = word.text.lower()\n",
    "\n",
    "    if word in word_dict:\n",
    "        word_dict[word] += 1\n",
    "    else:\n",
    "        word_dict[word] = 1\n",
    "\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Score Sentences ##########\n",
    "# (index, sentence, score)\n",
    "\n",
    "sentences = []\n",
    "\n",
    "sentence_score = 0\n",
    "\n",
    "for i, sentence in enumerate(doc.sents):\n",
    "    for word in sentence:\n",
    "        word = word.text.lower()\n",
    "        sentence_score += word_dict[word]\n",
    "\n",
    "    sentences.append((1, sentence.text.replace(\"\\n\", \" \"), sentence_score/len(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to,” said Thrun, in an interview with Recode earlier this week.\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to,” said Thrun, in an interview with Recode earlier this week. When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. \n"
     ]
    }
   ],
   "source": [
    "########## Sort Sentences By Importance ##########\n",
    "sorted_sentence = sorted(sentences, key = lambda x: -x[2])\n",
    "\n",
    "top_three = sorted(sorted_sentence[:3], key = lambda x: x[0])\n",
    "\n",
    "summary_text = \"\"\n",
    "\n",
    "for sentence in top_three:\n",
    "    summary_text += sentence[1] + \" \"\n",
    "print(summary_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
