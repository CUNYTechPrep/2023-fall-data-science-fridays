{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Ioannou_Georgios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright Â© 2023 by Georgios Ioannou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 align=\"center\"> Natural Language Processing </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluating Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# LIBRARIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zacharydesario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries.\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from nltk import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB data set into a pandas data frame.\n",
    "\n",
    "# Read the file IMDB Dataset.csv and load the data. GOOD, CLEAN, ORGANIZED DATASET.\n",
    "\n",
    "df = pd.read_csv(\"archive.zip\")\n",
    "\n",
    "# Print/Display/Return the first 5 rows of the file IMDB Dataset.csv to make sure the file was loaded successfully.\n",
    "\n",
    "df.head()\n",
    "\n",
    "df = df.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\n",
    "    \"review\"\n",
    "].values  # Return a Numpy representation of the DataFrame to use in the train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\n",
    "    \"sentiment\"\n",
    "].values  # Return a Numpy representation of the DataFrame to use in the train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters).\n",
    "\n",
    "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "\n",
    "    # Replace all runs of whitespaces with no space.\n",
    "\n",
    "    s = re.sub(r\"\\s+\", \"\", s)\n",
    "\n",
    "    # Replace digits with no space.\n",
    "\n",
    "    s = re.sub(r\"\\d\", \"\", s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(X_train, y_train, x_val, y_val):\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    for sent in X_train:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != \"\":\n",
    "                word_list.append(word)\n",
    "\n",
    "    # corpus: A collection of words.\n",
    "\n",
    "    corpus = Counter(word_list)\n",
    "\n",
    "    # Sorting on the basis of most common words.\n",
    "\n",
    "    corpus_ = sorted(corpus, key=corpus.get, reverse=True)[:1000]\n",
    "\n",
    "    # Create a dictionary.\n",
    "\n",
    "    onehot_dict = {w: i + 1 for i, w in enumerate(corpus_)}\n",
    "\n",
    "    # Tokenize.\n",
    "\n",
    "    final_list_train, final_list_test = [], []\n",
    "\n",
    "    for sent in X_train:\n",
    "        final_list_train.append(\n",
    "            [\n",
    "                onehot_dict[preprocess_string(word)]\n",
    "                for word in sent.lower().split()\n",
    "                if preprocess_string(word) in onehot_dict.keys()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    for sent in x_val:\n",
    "        final_list_test.append(\n",
    "            [\n",
    "                onehot_dict[preprocess_string(word)]\n",
    "                for word in sent.lower().split()\n",
    "                if preprocess_string(word) in onehot_dict.keys()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    encoded_train = [1 if label == \"positive\" else 0 for label in y_train]\n",
    "\n",
    "    encoded_test = [1 if label == \"positive\" else 0 for label in y_val]\n",
    "\n",
    "    return (\n",
    "        np.array(final_list_train, dtype=object),\n",
    "        np.array(encoded_train),\n",
    "        np.array(final_list_test, dtype=object),\n",
    "        np.array(encoded_test),\n",
    "        onehot_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, vocab = tokenize(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review) :] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "# Check if a CUDA-capable GPU is available, and set 'device' accordingly.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Print the selected device (either 'cuda' or 'cpu') to the console.\n",
    "\n",
    "print(device)\n",
    "\n",
    "# Check if CUDA (GPU support) is available.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Print the number of available CUDA devices.\n",
    "\n",
    "    print(torch.cuda.device_count())\n",
    "\n",
    "    # Print the name of the first CUDA device.\n",
    "\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    # If no GPU is available, print a message.\n",
    "\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model):\n",
    "    word_seq = np.array(\n",
    "        [\n",
    "            vocab[preprocess_string(word)]\n",
    "            for word in text.split()\n",
    "            if preprocess_string(word) in vocab.keys()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    word_seq = np.expand_dims(word_seq, axis=0)\n",
    "\n",
    "    pad = torch.from_numpy(padding_(word_seq, 500))\n",
    "\n",
    "    inputs = pad.to(device)\n",
    "\n",
    "    batch_size = 1\n",
    "\n",
    "    h = model.init_hidden(batch_size)\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    output, h = model(inputs, h)\n",
    "\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class definition.\n",
    "\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        no_layers,\n",
    "        vocab_size,\n",
    "        output_dim,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        drop_prob=0.5,\n",
    "    ):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Embedding layers.\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layers.\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=no_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Dropout layer.\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Linear layer. (Fully Connected Layer)\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "        # Sigmoid layer.\n",
    "\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1]\n",
    "\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "\n",
    "        h0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
    "\n",
    "        hidden = (h0, c0)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(1001, 64)\n",
       "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model class parameters.\n",
    "\n",
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1  # Extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "# Create an instance of the SentimentRNN class.\n",
    "\n",
    "loaded_model = SentimentRNN(\n",
    "    no_layers, vocab_size, output_dim, hidden_dim, embedding_dim, drop_prob=0.5\n",
    ").to(device)\n",
    "\n",
    "# Load the trained model's parameters.\n",
    "\n",
    "loaded_model.load_state_dict(\n",
    "    torch.load(\"my_model_1.pt\", map_location=torch.device(device))\n",
    ")\n",
    "# loaded_model.load_state_dict(\n",
    "#     torch.load(\"my_model_2.pt\", map_location=torch.device(device))\n",
    "# )\n",
    "\n",
    "# Set the model to evaluation mode.\n",
    "\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9/10 IMDB Review\n",
    "# https://www.imdb.com/title/tt15398776/reviews?sort=curated&dir=asc&ratingFilter=9\n",
    "# REVIEWS FROM Oppenheimer (2023) NOT IN THE DATASET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New POSITIVE review:\n",
      " You'll have to have your wits about you and your brain fully switched on watching Oppenheimer as it could easily get away from a nonattentive viewer. This is intelligent filmmaking which shows it's audience great respect. It fires dialogue packed with information at a relentless pace and jumps to very different times in Oppenheimer's life continuously through it's 3 hour runtime. There are visual clues to guide the viewer through these times but again you'll have to get to grips with these quite quickly. This relentlessness helps to express the urgency with which the US attacked it's chase for the atomic bomb before Germany could do the same. An absolute career best performance from (the consistenly brilliant) Cillian Murphy anchors the film. This is a nailed on Oscar performance. In fact the whole cast are fantastic (apart maybe for the sometimes overwrought Emily Blunt performance). RDJ is also particularly brilliant in a return to proper acting after his decade or so of calling it in. The screenplay is dense and layered (I'd say it was a thick as a Bible), cinematography is quite stark and spare for the most part but imbued with rich, lucious colour in moments (especially scenes with Florence Pugh), the score is beautiful at times but mostly anxious and oppressive, adding to the relentless pacing. The 3 hour runtime flies by. All in all I found it an intense, taxing but highly rewarding watch. This is film making at it finest. A really great watch.\n",
      "\n",
      "Predicted sentiment is POSITIVE with a probability of 64.48707580566406%\n"
     ]
    }
   ],
   "source": [
    "new_positive_review = \"You'll have to have your wits about you and your brain fully switched on watching Oppenheimer as it could easily get away from a nonattentive viewer. This is intelligent filmmaking which shows it's audience great respect. It fires dialogue packed with information at a relentless pace and jumps to very different times in Oppenheimer's life continuously through it's 3 hour runtime. There are visual clues to guide the viewer through these times but again you'll have to get to grips with these quite quickly. This relentlessness helps to express the urgency with which the US attacked it's chase for the atomic bomb before Germany could do the same. An absolute career best performance from (the consistenly brilliant) Cillian Murphy anchors the film. This is a nailed on Oscar performance. In fact the whole cast are fantastic (apart maybe for the sometimes overwrought Emily Blunt performance). RDJ is also particularly brilliant in a return to proper acting after his decade or so of calling it in. The screenplay is dense and layered (I'd say it was a thick as a Bible), cinematography is quite stark and spare for the most part but imbued with rich, lucious colour in moments (especially scenes with Florence Pugh), the score is beautiful at times but mostly anxious and oppressive, adding to the relentless pacing. The 3 hour runtime flies by. All in all I found it an intense, taxing but highly rewarding watch. This is film making at it finest. A really great watch.\"\n",
    "\n",
    "print(\"New POSITIVE review:\\n\", new_positive_review)\n",
    "\n",
    "pro = predict_text(new_positive_review, loaded_model)\n",
    "status = \"positive\" if pro > 0.5 else \"negative\"\n",
    "pro = (1 - pro) if status == \"negative\" else pro\n",
    "\n",
    "print(f\"\\nPredicted sentiment is {status.upper()} with a probability of {pro * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5/10 IMDB Review\n",
    "# https://www.imdb.com/title/tt15398776/reviews?sort=curated&dir=asc&ratingFilter=5\n",
    "# REVIEWS FROM Oppenheimer (2023) NOT IN THE DATASET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New AVERAGE review:\n",
      " This must be the most overrated film of the year.Like every other typical American biographical movie, it glorifies its subject. According to Nolan, Oppenheimer is the most important person who ever lived. Really?The movie contains at least 1.5 hours of uninteresting courtroom drama.There are far too many characters.Not a single compelling dialogue about the moral impact of the bomb.The continuous music propels the film, but as a result there is zero emotional impact when needed.Excellent acting by all, though, and occasional nice directorial effects.Most ridiculous moment of the film: While having sex with Oppenheimer, Florence Pughs character randomly selects a sentence in a book in Sanskrit which just happens to be the infamous \"Now I Am Become Death, the Destroyer of Worlds\" quote.\n",
      "\n",
      "Predicted sentiment is POSITIVE with a probability of 50.37773847579956%\n"
     ]
    }
   ],
   "source": [
    "new_average_review = 'This must be the most overrated film of the year.Like every other typical American biographical movie, it glorifies its subject. According to Nolan, Oppenheimer is the most important person who ever lived. Really?The movie contains at least 1.5 hours of uninteresting courtroom drama.There are far too many characters.Not a single compelling dialogue about the moral impact of the bomb.The continuous music propels the film, but as a result there is zero emotional impact when needed.Excellent acting by all, though, and occasional nice directorial effects.Most ridiculous moment of the film: While having sex with Oppenheimer, Florence Pughs character randomly selects a sentence in a book in Sanskrit which just happens to be the infamous \"Now I Am Become Death, the Destroyer of Worlds\" quote.'\n",
    "\n",
    "print(\"New AVERAGE review:\\n\", new_average_review)\n",
    "\n",
    "pro = predict_text(new_average_review, loaded_model)\n",
    "status = \"positive\" if pro > 0.5 else \"negative\"\n",
    "pro = (1 - pro) if status == \"negative\" else pro\n",
    "\n",
    "print(f\"\\nPredicted sentiment is {status.upper()} with a probability of {pro * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/10 IMDB Review\n",
    "# https://www.imdb.com/title/tt15398776/reviews?sort=curated&dir=asc&ratingFilter=1\n",
    "# REVIEWS FROM Oppenheimer (2023) NOT IN THE DATASET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New NEGATIVE review:\n",
      " After you watch Oppenheimer and leave the movie theater, you ask yourself: \"What was the point of it?\" Unfortunately the answer is not clear at all. I strongly beleive that a movie about the scientist who played a key role in the creation of the atomic bomb should unquestionably emphasize on the consequences of this invention and send strong peaceful messages to the audience! Especially in the world we live in today! However, it looks like in the new movie Oppenheimer they almost ignore the tragic facts that hundreds of thousands of innocent people died from the first 2 bombs and that the whole world changed after Oppenheim's invention, which was obviously the biggest drama in this one human being's life. They emphasize and build the story in the movie around the fact that Oppenheimer was later accused of being a spy of the Soviets (who were allies to the Americans in WW2...).I feel sad that this will be an award winning movie and that most people in the audience will say (or feel they have to say) \"A! Oh!! Great movie!\", because the director, writers, cast are famous and the topic is important - all these superlatives about a somewhat boring movie, decorated with some nudity (not clear either why this was needed and related to the topic) and overall pointless movie... Or, at least, much more pointless than it should be!\n",
      "\n",
      "Predicted sentiment is NEGATIVE with a probability of 89.04632180929184%\n"
     ]
    }
   ],
   "source": [
    "new_negative_review = 'After you watch Oppenheimer and leave the movie theater, you ask yourself: \"What was the point of it?\" Unfortunately the answer is not clear at all. I strongly beleive that a movie about the scientist who played a key role in the creation of the atomic bomb should unquestionably emphasize on the consequences of this invention and send strong peaceful messages to the audience! Especially in the world we live in today! However, it looks like in the new movie Oppenheimer they almost ignore the tragic facts that hundreds of thousands of innocent people died from the first 2 bombs and that the whole world changed after Oppenheim\\'s invention, which was obviously the biggest drama in this one human being\\'s life. They emphasize and build the story in the movie around the fact that Oppenheimer was later accused of being a spy of the Soviets (who were allies to the Americans in WW2...).I feel sad that this will be an award winning movie and that most people in the audience will say (or feel they have to say) \"A! Oh!! Great movie!\", because the director, writers, cast are famous and the topic is important - all these superlatives about a somewhat boring movie, decorated with some nudity (not clear either why this was needed and related to the topic) and overall pointless movie... Or, at least, much more pointless than it should be!'\n",
    "\n",
    "print(\"New NEGATIVE review:\\n\", new_negative_review)\n",
    "\n",
    "pro = predict_text(new_negative_review, loaded_model)\n",
    "status = \"positive\" if pro > 0.5 else \"negative\"\n",
    "pro = (1 - pro) if status == \"negative\" else pro\n",
    "\n",
    "print(f\"\\nPredicted sentiment is {status.upper()} with a probability of {pro * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
